hadoop：
依赖：
	java-1.8.0-openjdk
	java-1.8.0-openjdk-devel
安装：
	tar -xf hadoop-2.7.6.tar.gz  # 该软件包在Hadoop.zip包内
	mv hadoop-2.7.6/ /usr/local/hadoop
验证：
	/usr/local/hadoop/bin/hadoop  # 输入后会有命令的帮助提示
	报错：Error: JAVA_HOME is not set and could not be found.
	解决：
		vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh
			25 export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre"
			33 export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"
		# 将25修改为本机java的启动路径，33修改为本机hadoop的启动脚本路径
需配置hosts：
	192.168.1.10  hadoop01
	192.168.1.11  node1
	192.168.1.12  node2
	192.168.1.13  node3
测试hadoop：
	pwd
		/usr/local/hadoop
	mkdir idata
	cp *.txt idata/
	ls idata/
		LICENSE.txt  NOTICE.txt  README.txt
	./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount idata odata
	ls odata
		part-r-00000  _SUCCESS
	cat part-r-00000
	# 此方法为计算上方三个文本文件中单词出现的次数
	### 下方bash指令用于对单词个数进行排序
	cat odata/part-r-00000 | awk '{print $2}' | sort -n

hadoop配置完全分布式：
需配置四个文件：
hadoop-env.sh core-site.xml hdfs-site.xml slaves
格式xml
    <property>
        <name></name>
        <value></value>
        <description></description>
    </property>

配置：
vim /usr/local/hadoop/etc/hadoop/core-site.xml
<configuration>
    <property>
        <name>fs.defaultFS</name>  # 定义namenode
        <value>hdfs://hadoop01:9000</value>
        <description>hadoop default filesystem</description>  # 注释
    </property>
    <property>
        <name>hadoop.tmp.dir</name>  # 定义hadoop所有数据的存放目录
        <value>/var/hadoop</value>
    </property>
</configuration>
###上述选项可以在下面网址找到：
    https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-common/core-default.xml
##########
mkdir /var/hadoop  # 由于上面配置文件指定了数据存放目录，需要在这创建
vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop01:50070</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop01:50090</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
</configuration>
vim /usr/local/hadoop/etc/hadoop/slaves
    node1
    node2
    node3
for i in node{1..3}; do rsync -aSH --delete /usr/local/hadoop/ $i:/usr/local/hadoop/  -e 'ssh' & done



/usr/local/hadoop/bin/hdfs namenode -format  # 格式化
/usr/local/hadoop/sbin/start-dfs.sh  # 启动
测试：
jps
/usr/local/hadoop/bin/hdfs dfsadmin -report



