RAID与ceph会相互影响，尽量不要同时使用
ceph所有数据都有三个副本
Ceph组件：
	OSDs
		--存储设备
		--无限个
	Monitors
		--集群监控组件
		--最少3个
	RGW
		--对象存储网关
		--以对象存储的方式访问
	MDSs
		--存放文件系统的元数据
		--块存储，文件系统存储，对象存储（百度网盘）
	Client
		--客户端（ceph-common）

部署前的准备工作：
	client:
		eth0:192.168.4.10
		yum:rhel7&ceph-osd,mon,toosl(这三个yum源在package下的rhcs**.iso)
		host:所有主机IP hostname
	node1:  # 批量远程操作主机服务器
		eth0:192.168.4.11
		yum:rhel7&ceph-osd,mon,toosl(这三个yum源在package下的rhcs**.iso)
		host:所有主机IP hostname
		ssh-keygen:远程所有主机都无需密码（ssh-copy-id），用于批量部署
			# ssh-copy-id IP & ssh-copy-id hostname
	node2:
		eth0:192.168.4.12
		yum:rhel7&ceph-osd,mon,toosl(这三个yum源在package下的rhcs**.iso)
		host:所有主机IP hostname
	node3:
		eth0:192.168.4.13
		yum:rhel7&ceph-osd,mon,toosl(这三个yum源在package下的rhcs**.iso)
		host:所有主机IP hostname
	真机:
		部署ftp发布rhcs2.0-rhosp9-20161113-x86_64.iso
		部署时间服务器，确保4台虚拟机（client，node1-3）时间一致

node1-3:
	yum -y install ceph-deploy  # 该软件是批量部署ceph的python脚本
		# 该软件需要以上准备的环境，无密码登录ceph-osd，hosts...等
	mkdir /ceph-cluster  # 新建ceph工作目录，ceph-deploy该软件只能在工作目录里执行
	cd /ceph-cluster
	ceph-deploy new node1 node2 node3  # 将node1-3作为mon
	ceph-deploy install node1 node2 node3  # 安装ceph有关的所有包装入node1-3
		# 包括osd，mon，mds...等
	ceph-deploy mon create-initial  # 初始化mon服务，将node1-3中的ceph-mon@xx.service的服务启动并设置开机自启
	ceph -s  # 查看当前集群状态
	parted  /dev/vdb  mklabel  gpt;parted  /dev/vdb  mkpart primary  1M  50%;sleep 1;parted  /dev/vdb  mkpart primary  50%  100%  # node1-3
		# 将vdb均分成两个分区
	chown ceph.ceph /dev/vdb1;chown ceph.ceph /dev/vdb2  # node1-3
		# 临时将vdb1-2的所有者和所属组改为ceph
	vim /etc/udev/rules.d/70-vdb.rules  # 永久改变vdb1-2设备的所有者和所属组 node1-3
		ENV{DEVNAME}=="/dev/vdb1",OWNER="ceph",GROUP="ceph"
		ENV{DEVNAME}=="/dev/vdb2",OWNER="ceph",GROUP="ceph"
	ceph-deploy disk zap node1:vdc node1:vdd  #
	ceph-deploy disk zap node2:vdc node2:vdd  # --将node1-3的vdc&vdd磁盘格式化，此操作只需在node1上的工作目录进行
	ceph-deploy disk zap node3:vdc node3:vdd  #
	ceph-deploy osd create node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2
		# 将主机node1的vdc作为osd，node1的/dev/vdb1作为vdc的缓存盘；node1的vdd作为osd，node1的/dev/vdb2作为vdd的缓存盘
		# 指令执行完会出现ceph-osd@0.service，ceph-osd@1.service的服务，并且是启动的，可以用systemctl status servername查看状态
	ceph-deploy osd create node2:vdc:/dev/vdb1 node2:vdd:/dev/vdb2
	ceph-deploy osd create node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb2
	ceph osd lspools  # 查看共享池
		0 rbd,  # rbd为共享池的名称，一个共享池可以制作多个共享镜像
	# 至此ceph-osd集群部署完毕
node1-3：
	rbd create demo-image --image-feature  layering --size 10G  # 在rbd池中创建名为demo-image的镜像--image-feature后面接需要添加的功能，大小10G
	rbd create rbd/image --image-feature  layering --size 10G  # 与上一条命令类似，rbd/image指明是在rbd池中创建名为image的镜像
		# 上述命令在所有mon服务器上都可以执行，只需要在一台机器执行一次，其他主机自动同步
	rbd list  # 显示rbd中的image
	rbd info (image|demo-image)  # 查看image信息
	rbd resize --size 7G image --allow-shrink  # 缩小image空间到7G，--allow-shrink该参数需要
	rbd resize --size 15G image  # 将image空间增加到15G
	# 到此服务端准备完毕

client：
	集群服务器本机访问：
		rbd map image  # 会自动生成/dev/rdb*的磁盘设备，该设备对应image镜像
			# 此时/dev/rbd*与普通硬盘设备无差，可以格式化可挂载
		rbd unmap image  # 可以卸载已挂载上的镜像image，"image"也可以写成/dev/rbd0，只要是对应镜像
	通过KRBD访问：
		yum -y install ceph-common  # 安装需要的软件包
	node1：
		需要到mon服务器的/etc/ceph/目录下：
			# eph.client.admin.keyring & ceph.conf将这两个文件cp到client:/etc/ceph/
			scp /etc/ceph/ceph.c* client:/etc/ceph/  # 命令
		rbd map image  # 安装完软件配置好conf和密码连接文件，可以与上方本地连接一致的命令进行挂载
			# 此时/dev/rbd*与普通硬盘设备无差，可以格式化可挂载
		rbd unmap image  # 与上方本地挂载一致
		rbd showmapped  # 查看设备路径对应的镜像
			id pool image snap device    
			0  rbd  image -    /dev/rbd0

创建镜像快照：
	
























